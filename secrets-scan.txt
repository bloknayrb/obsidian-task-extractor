./.kiro/specs/debug-logging/design.md:37:  category: 'file-processing' | 'llm-call' | 'task-creation' | 'service-detection' | 'validation' | 'error';
./.kiro/specs/debug-logging/design.md:130:- **task-creation**: Task note generation and file operations
./.kiro/specs/debug-logging/design.md:153:logger.log('info', 'task-creation', 'Created task note', { filePath: 'Tasks/new-task.md' }, correlationId);
./.kiro/specs/flexible-frontmatter-filtering/design.md:12:2. **Task Processing** (`src/task-processor.ts`): Update filtering logic to use configurable frontmatter field
./.kiro/specs/flexible-frontmatter-filtering/design.md:83:**Processing Logic**: `src/task-processor.ts`
./.kiro/specs/flexible-frontmatter-filtering/tasks.md:9:  - Modify `onFileChanged()` method in `src/task-processor.ts` to use `this.settings.triggerFrontmatterField` instead of hardcoded "Type"
./.kiro/specs/obsidian-api-compliance/design.md:16:### 1. Frontmatter Processing (src/task-processor.ts)
./.kiro/steering/structure.md:14:- **src/task-processor.ts**: Task processing, extraction, and file handling (280+ lines)
./.kiro/steering/structure.md:33:- **TaskProcessor** (src/task-processor.ts): Debounced processing, batch operations
./main.js:75:  if (settings.provider && ["openai", "anthropic", "ollama", "lmstudio"].includes(settings.provider)) {
./main.js:340:    if (["openai", "anthropic"].includes(provider) && !this.settings.apiKey) {
./main.js:368:          case "anthropic":
./main.js:536:    const endpoint = "https://api.anthropic.com/v1/messages";
./main.js:550:      provider: "anthropic",
./main.js:568:          "anthropic-version": "2023-06-01"
./main.js:578:          provider: "anthropic",
./main.js:591:        provider: "anthropic",
./main.js:606:        provider: "anthropic",
./main.js:822:      } else if (provider === "anthropic") {
./main.js:904:      provider: "anthropic",
./main.js:915:      provider: "anthropic",
./main.js:920:    const cacheKey = `anthropic-${this.settings.apiKey.slice(-4)}`;
./main.js:923:      provider: "anthropic",
./main.js:933:      anthropic: ["claude-3-5-sonnet-20241022", "claude-3-haiku-20240307"],
./main.js:976:// src/task-processor.ts
./main.js:1435:        this.log("info", "task-creation", "Processing multi-task extraction result", {
./main.js:1446:            this.log("error", "task-creation", "Failed to create individual task note", {
./main.js:1453:        this.log("info", "task-creation", "Multi-task creation completed", {
./main.js:1463:        this.log("info", "task-creation", "Processing single-task extraction result", {
./main.js:1472:      this.log("error", "task-creation", "Task extraction handling failed", {
./main.js:1676:    this.log("info", "task-creation", "Creating task note", {
./main.js:1717:      this.log("info", "task-creation", "Task note created successfully", {
./main.js:1726:      this.log("error", "task-creation", "Failed to create task note", {
./main.js:1902:    const statusEl = containerEl.createEl("div", { cls: "task-extractor-status" });
./main.js:1904:    new import_obsidian3.Setting(containerEl).setName("Provider").setDesc("Choose LLM provider. Local providers (Ollama/LM Studio) require the service to be running.").addDropdown((cb) => cb.addOption("openai", "OpenAI").addOption("anthropic", "Anthropic").addOption("ollama", "Ollama (Local)").addOption("lmstudio", "LM Studio (Local)").setValue(this.settings.provider).onChange((v) => {
./main.js:1911:    if (["openai", "anthropic"].includes(this.settings.provider)) {
./main.js:1912:      new import_obsidian3.Setting(containerEl).setName("API Key").setDesc("Your API key for the selected provider. Models will be loaded automatically once entered.").addText((text) => text.setPlaceholder("sk-... or claude-...").setValue(this.settings.apiKey).onChange((v) => {
./main.js:1935:    } else if (["openai", "anthropic"].includes(provider) && this.settings.apiKey) {
./main.js:1964:      anthropic: "claude-3-haiku-20240307",
./main.js:2051:      const fieldContainer = containerEl.createDiv({ cls: "task-extractor-field" });
./main.js:2147:    const controlContainer = setting.controlEl.createDiv({ cls: "task-extractor-slider-input-container" });
./main.js:2150:      cls: "task-extractor-slider"
./main.js:2158:      cls: "task-extractor-number-input"
./main.js:2213:          cls: "task-extractor-status-success"
./main.js:2218:          cls: "task-extractor-status-error"
./main.js:2225:          cls: "task-extractor-status-success"
./main.js:2230:          cls: "task-extractor-status-error"
./main.js:2499:      const sensitiveFields = ["apiKey", "token", "password", "secret", "key"];
./main.ts:16:import { TaskProcessor } from './src/task-processor';
./main.ts:162:  async fetchCloudModels(provider: 'openai' | 'anthropic'): Promise<string[]> {
./manifest.json:2:  "id": "task-extractor",
./obsidian_task_extractor.txt:7:Settings will include a provider choice (`openai | anthropic | ollama | lmstudio`), model configuration, and a toggle for inline checklist creation.
./package-lock.json:2:  "name": "obsidian-task-extractor",
./package-lock.json:8:      "name": "obsidian-task-extractor",
./package-lock.json:2303:      "resolved": "https://registry.npmjs.org/queue-microtask/-/queue-microtask-1.2.3.tgz",
./package.json:2:  "name": "obsidian-task-extractor",
./README.md:54:2. Extract to `.obsidian/plugins/task-extractor/` in your vault
./README.md:60:git clone https://github.com/bryanjkolb/obsidian-task-extractor.git
./README.md:61:cd obsidian-task-extractor
./README.md:91:- Get API key from [Anthropic Console](https://console.anthropic.com)
./README.md:302:- Check API key format (starts with `sk-` for OpenAI)
./README.md:335:- **task-creation**: Task note creation and validation
./README.md:359:- **src/task-processor.ts** - Debounced file processing and batch operations
./README.md:392:obsidian-task-extractor/
./README.md:397:│   ├── task-processor.ts# File processing and task extraction
./README.md:441:- **Issues**: [GitHub Issues](https://github.com/bryanjkolb/obsidian-task-extractor/issues)
./README.md:442:- **Discussions**: [GitHub Discussions](https://github.com/bryanjkolb/obsidian-task-extractor/discussions)
./README.md:472:- **Structured Logging**: Categorized logs (file-processing, llm-call, task-creation, validation, error) with contextual data
grep: input file ‘./secrets-scan.txt’ is also the output
./src/debug-logger.ts:9:  category: 'file-processing' | 'llm-call' | 'task-creation' | 'service-detection' | 'validation' | 'error';
./src/debug-logger.ts:405:      const sensitiveFields = ['apiKey', 'token', 'password', 'secret', 'key'];
./src/llm-providers.ts:200:    if (['openai', 'anthropic'].includes(provider) && !this.settings.apiKey) {
./src/llm-providers.ts:235:          case 'anthropic':
./src/llm-providers.ts:419:    const endpoint = 'https://api.anthropic.com/v1/messages';
./src/llm-providers.ts:435:      provider: 'anthropic',
./src/llm-providers.ts:454:          'anthropic-version': '2023-06-01'
./src/llm-providers.ts:465:          provider: 'anthropic',
./src/llm-providers.ts:480:        provider: 'anthropic',
./src/llm-providers.ts:496:        provider: 'anthropic',
./src/llm-providers.ts:703:  async fetchCloudModels(provider: 'openai' | 'anthropic'): Promise<string[]> {
./src/llm-providers.ts:737:      } else if (provider === 'anthropic') {
./src/llm-providers.ts:832:      provider: 'anthropic',
./src/llm-providers.ts:845:      provider: 'anthropic',
./src/llm-providers.ts:852:    const cacheKey = `anthropic-${this.settings.apiKey.slice(-4)}`;
./src/llm-providers.ts:856:      provider: 'anthropic',
./src/llm-providers.ts:867:      anthropic: ['claude-3-5-sonnet-20241022', 'claude-3-haiku-20240307'],
./src/settings.ts:149:    const statusEl = containerEl.createEl('div', { cls: 'task-extractor-status' });
./src/settings.ts:157:        .addOption('anthropic', 'Anthropic')
./src/settings.ts:173:    if (['openai', 'anthropic'].includes(this.settings.provider)) {
./src/settings.ts:178:          .setPlaceholder('sk-... or claude-...')
./src/settings.ts:218:    } else if (['openai', 'anthropic'].includes(provider) && this.settings.apiKey) {
./src/settings.ts:225:        const availableModels = await this.llmProvider.fetchCloudModels(provider as 'openai' | 'anthropic');
./src/settings.ts:264:      anthropic: 'claude-3-haiku-20240307',
./src/settings.ts:417:      const fieldContainer = containerEl.createDiv({ cls: 'task-extractor-field' });
./src/settings.ts:538:    const controlContainer = setting.controlEl.createDiv({ cls: 'task-extractor-slider-input-container' });
./src/settings.ts:543:      cls: 'task-extractor-slider'
./src/settings.ts:554:      cls: 'task-extractor-number-input'
./src/settings.ts:639:          cls: 'task-extractor-status-success'
./src/settings.ts:644:          cls: 'task-extractor-status-error'
./src/settings.ts:651:          cls: 'task-extractor-status-success'
./src/settings.ts:656:          cls: 'task-extractor-status-error'
./src/task-processor.ts:33:    category: 'file-processing' | 'llm-call' | 'task-creation' | 'service-detection' | 'validation' | 'error',
./src/task-processor.ts:47:    category: 'file-processing' | 'llm-call' | 'task-creation' | 'service-detection' | 'validation' | 'error',
./src/task-processor.ts:563:        this.log('info', 'task-creation', 'Processing multi-task extraction result', { 
./src/task-processor.ts:575:            this.log('error', 'task-creation', 'Failed to create individual task note', { 
./src/task-processor.ts:583:        this.log('info', 'task-creation', 'Multi-task creation completed', { 
./src/task-processor.ts:595:        this.log('info', 'task-creation', 'Processing single-task extraction result', { 
./src/task-processor.ts:605:      this.log('error', 'task-creation', 'Task extraction handling failed', { 
./src/task-processor.ts:850:    this.log('info', 'task-creation', 'Creating task note', { 
./src/task-processor.ts:905:      this.log('info', 'task-creation', 'Task note created successfully', { 
./src/task-processor.ts:914:      this.log('error', 'task-creation', 'Failed to create task note', { 
./src/types.ts:22:  provider: 'openai' | 'anthropic' | 'ollama' | 'lmstudio';
./src/types.ts:138:  if (settings.provider && ['openai', 'anthropic', 'ollama', 'lmstudio'].includes(settings.provider)) {
./TASK_LIST.md:14:**Priority**: Critical | **Effort**: Medium | **Files**: `src/task-processor.ts`  
./TASK_LIST.md:25:**Priority**: High | **Effort**: Medium | **Files**: `src/task-processor.ts`  
./TASK_LIST.md:39:**Priority**: High | **Effort**: Low | **Files**: `src/task-processor.ts`, `src/types.ts`  
./TASK_LIST.md:57:**Priority**: Medium | **Effort**: Low | **Files**: `src/task-processor.ts`  
./TASK_LIST.md:71:**Priority**: Medium | **Effort**: Medium | **Files**: `src/task-processor.ts`, `src/llm-providers.ts`  
./TASK_LIST.md:85:**Priority**: Medium | **Effort**: Low | **Files**: `src/task-processor.ts`  
./TASK_LIST.md:103:**Priority**: Low | **Effort**: Minimal | **Files**: `src/task-processor.ts`  
./TASK_LIST.md:189:- ✅ `src/task-processor.ts` - Eliminated code duplication (Task 1)
./TASK_LIST.md:192:- 🔄 `src/task-processor.ts` - Race condition fixes (Task 2)
./test/debug-integration.test.ts:2:import { TaskProcessor } from '../src/task-processor';
./test/integration.test.ts:3:import { TaskProcessor } from '../src/task-processor';
./test/integration.test.ts:142:        apiKey: 'sk-test',
./test/README.md:13:### 2. Task Processing Logic (`test/task-processor.test.ts`)
./test/task-processor.test.ts:2:import { TaskProcessor } from '../src/task-processor';
./test/types.test.ts:138:        apiKey: 'sk-test123',
./test/types.test.ts:147:      expect(validated.apiKey).toBe('sk-test123');
./vitest-output.txt:3:[7m[1m[36m RUN [39m[22m[27m [36mv1.6.1[39m [90mC:/Users/bkolb/Documents/GitHub/obsidian-task-extractor[39m
./vitest-output.txt:23: [32mΓ£ô[39m test/task-processor.test.ts[2m > [22mTaskProcessor - Frontmatter Field Configuration[2m > [22mconfigured frontmatter field usage[2m > [22mshould use configured frontmatter field instead of hardcoded "Type"
./vitest-output.txt:24: [32mΓ£ô[39m test/task-processor.test.ts[2m > [22mTaskProcessor - Frontmatter Field Configuration[2m > [22mconfigured frontmatter field usage[2m > [22mshould not process files when custom field does not match
./vitest-output.txt:25: [32mΓ£ô[39m test/task-processor.test.ts[2m > [22mTaskProcessor - Frontmatter Field Configuration[2m > [22mconfigured frontmatter field usage[2m > [22mshould fallback to "Type" field when configured field is invalid
./vitest-output.txt:26: [32mΓ£ô[39m test/task-processor.test.ts[2m > [22mTaskProcessor - Frontmatter Field Configuration[2m > [22mconfigured frontmatter field usage[2m > [22mshould handle case-insensitive comparison with custom field
./vitest-output.txt:27: [32mΓ£ô[39m test/task-processor.test.ts[2m > [22mTaskProcessor - Frontmatter Field Configuration[2m > [22mgetUnprocessedFiles with custom frontmatter field[2m > [22mshould filter files using configured frontmatter field
./vitest-output.txt:28: [32mΓ£ô[39m test/task-processor.test.ts[2m > [22mTaskProcessor - Frontmatter Field Configuration[2m > [22mfrontmatter field validation[2m > [22mshould validate frontmatter field names
./vitest-output.txt:29: [32mΓ£ô[39m test/task-processor.test.ts[2m > [22mTaskProcessor - Frontmatter Field Configuration[2m > [22mbackward compatibility[2m > [22mshould work with existing configurations that do not specify frontmatter field
